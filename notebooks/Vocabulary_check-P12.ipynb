{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be211528-c9d1-424c-9ee8-09840a959842",
   "metadata": {},
   "source": [
    "    Author: Saman Firdaus Chishti   |   chishti@gfz-potsdam.de\n",
    "\n",
    "    Start date: 29-02-2024\n",
    "\n",
    "    \n",
    "**Description:** This set of code has been developed to check whether all the values entered in a Heatflow database adhere to a controlled vocabulary and proper structure. It generates an error message for each entry where the value entered is out of bounds and does not meet the assigned criteria. The code also enables checking the vocabulary for multiple values entered in a single column for a particular Heatflow data entry.\n",
    "This is in compliance with the paper by Fuchs et al. (2023) titled \"[Quality-assurance of heat-flow data: The new structure and evaluation scheme of the IHFC Global Heat Flow Database](https://doi.org/10.1016/j.tecto.2023.229976),\" published in Tectonophysics 863: 229976. Also revised for the newer release 2024.\n",
    "\n",
    "The code is intended to be published on the GFZ website for the global scientific community to check if any Heatflow dataset adheres to the data structure described in the aforementioned scientific paper. It's a recommended prerequisite before calculating 'Quality Scores' for a given Heatflow dataset. The code for calculating 'Quality Scores' is provided in a separate document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05274602",
   "metadata": {},
   "source": [
    "![Vocab Image](Graphics//Vocab.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ae9857-5d00-4f40-8ef9-21522f20e6ee",
   "metadata": {},
   "source": [
    "# 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f086e405-89c5-428e-a43c-40dc13c9aa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 484 ms\n",
      "Wall time: 491 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import openpyxl\n",
    "import warnings\n",
    "import glob\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc73f416-d561-4427-b644-f320b289cb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbad17d0-1555-4519-a87c-d3d11364d22c",
   "metadata": {},
   "source": [
    "# 2. Preparing dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740c08b",
   "metadata": {},
   "source": [
    "## 2.1. Convert to .csv utf-8 format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdc4dac",
   "metadata": {},
   "source": [
    "    [Description]: Convert all the Heatflow database files within a folder in the usual Excel sheet format to .csv format. Which is easily compatible for the functions mentioned below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5653ba7-9174-4a44-8176-6fa48616a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert2UTF8csv(folder_path):    \n",
    "    excel_files = glob.glob(os.path.join(folder_path, '*.xlsx'))\n",
    "\n",
    "    for excel_file_path in excel_files:\n",
    "        if excel_file_path.endswith('_vocab_check.xlsx') or excel_file_path.endswith('_scores_result.xlsx'):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            excel_file = pd.ExcelFile(excel_file_path, engine='openpyxl')\n",
    "            \n",
    "            data_list_sheet = excel_file.parse('data list')\n",
    "            \n",
    "            output_csv_file = os.path.splitext(excel_file_path)[0] + '.csv'\n",
    "            \n",
    "            data_list_sheet.to_csv(output_csv_file, index=False, encoding='utf-8')\n",
    "            \n",
    "            del data_list_sheet\n",
    "            del excel_file\n",
    "            \n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing {excel_file_path}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred while processing {excel_file_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903f7419-e1da-44fa-93eb-4b0c57870013",
   "metadata": {},
   "source": [
    "## 2.2. Extract 10K entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92a7cfc",
   "metadata": {},
   "source": [
    "    [Disclaimer]: Only required for very large database with more than 10,000 entry.\n",
    "    \n",
    "    [Description]: To prepare segments of a large Heatflow database file, such as Global Heatflow Database 2024 release. Which has more than 90,000 Heatflow data entries. Each segment/ chunk would have 10,000 entries or rows. The segmentation helps run the program functions faster in generating results output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38bfc255-69dc-41e0-ab94-3058dbfb7d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract10K(df,start):\n",
    "    df_first_7_rows = df.head(7)\n",
    "    df = remove_rows(df)\n",
    "\n",
    "    df['ID'] = df['ID'].astype(float)\n",
    "    end = start + 10000\n",
    "\n",
    "    filtered_df = df[(df['ID'] >= start) & (df['ID'] <= end)]\n",
    "\n",
    "    appended_df = pd.concat([df_first_7_rows, filtered_df], ignore_index=True)\n",
    "    appended_df.to_csv(f\"chunk{start}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67cbfed-94ad-4703-a66b-3bd4bd72dba3",
   "metadata": {},
   "source": [
    "# 3. Controlled vocabulary\n",
    "## 3.1. Assigning columns with similar data types to specific list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "921e77c8-ca29-4131-9564-e6d0cfb694e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NumC = ['P1','P2','P4','P5','P6','P10','P11','C4','C5','C6','C22','C23','C24','C27','C28','C29','C30','C33','C34','C37','C39','C40','C47']\n",
    "StrC = ['P7','P9','P12','P13','C3','C11','C12','C13','C14','C15','C17','C18','C19','C21','C31','C32','C35','C36','C41','C42','C43','C44','C45','C46','C48']\n",
    "DateC = ['C38']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83de2f4a-2a44-4d2b-8276-287662725379",
   "metadata": {},
   "source": [
    "## 3.2. Numeric value sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e6cb62",
   "metadata": {},
   "source": [
    "    [Description]: Assign permissible value ranges for columns that store numeric values. The Allowed range of values are taken from \"Appendix A. Structure and field definitions of the IHFC Global Heat Flow Database\" in the aforementioned paper. Also revised for the newer release 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a38c444-1f07-4398-8ec6-29c38afeafd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_data = {\n",
    "    #'ID': ['P1','P2','P4','P5','P6','P10','P11','C4','C5','C6','C22','C23','C24','C27','C28','C29','C30','C33','C34','C37','C39','C40','C47'],\n",
    "    'Min': [-999999.9,0,-90.00000,-180.00000,-12000,-12000,-12000,0,0,0,0,0,-9.99,-99999.99,-99999.99,-99999.99,-99999.99,0,0,0,0,0,0],\n",
    "    'Max': [999999.9,999999.9,90.00000,180.00000,9000,9000,9000,19999.9,19999.9,999.9,99.99,99.99,99.99,99999.99,99999.99,99999.99,99999.99,99999,99999,999999,99.99,99.99,9999]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5ab7eaa-62ce-4f87-85bb-d5a62a569a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "ndf = pd.DataFrame(num_data, index=NumC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400e3957-6fa9-4ded-9d54-f2b9669c55d3",
   "metadata": {},
   "source": [
    "### 3.2.1. Pivot the DataFrame: Rows become columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b7b34661-9627-4f6e-93cd-3c5be45a8116",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P1</th>\n",
       "      <th>P2</th>\n",
       "      <th>P4</th>\n",
       "      <th>P5</th>\n",
       "      <th>P6</th>\n",
       "      <th>P10</th>\n",
       "      <th>P11</th>\n",
       "      <th>C4</th>\n",
       "      <th>C5</th>\n",
       "      <th>C6</th>\n",
       "      <th>...</th>\n",
       "      <th>C27</th>\n",
       "      <th>C28</th>\n",
       "      <th>C29</th>\n",
       "      <th>C30</th>\n",
       "      <th>C33</th>\n",
       "      <th>C34</th>\n",
       "      <th>C37</th>\n",
       "      <th>C39</th>\n",
       "      <th>C40</th>\n",
       "      <th>C47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Min</th>\n",
       "      <td>-999999.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>-180.0</td>\n",
       "      <td>-12000.0</td>\n",
       "      <td>-12000.0</td>\n",
       "      <td>-12000.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>-99999.99</td>\n",
       "      <td>-99999.99</td>\n",
       "      <td>-99999.99</td>\n",
       "      <td>-99999.99</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Max</th>\n",
       "      <td>999999.9</td>\n",
       "      <td>999999.9</td>\n",
       "      <td>90.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>9000.0</td>\n",
       "      <td>19999.9</td>\n",
       "      <td>19999.9</td>\n",
       "      <td>999.9</td>\n",
       "      <td>...</td>\n",
       "      <td>99999.99</td>\n",
       "      <td>99999.99</td>\n",
       "      <td>99999.99</td>\n",
       "      <td>99999.99</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>99999.0</td>\n",
       "      <td>999999.0</td>\n",
       "      <td>99.99</td>\n",
       "      <td>99.99</td>\n",
       "      <td>9999.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           P1        P2    P4     P5       P6      P10      P11       C4  \\\n",
       "Min -999999.9       0.0 -90.0 -180.0 -12000.0 -12000.0 -12000.0      0.0   \n",
       "Max  999999.9  999999.9  90.0  180.0   9000.0   9000.0   9000.0  19999.9   \n",
       "\n",
       "          C5     C6  ...       C27       C28       C29       C30      C33  \\\n",
       "Min      0.0    0.0  ... -99999.99 -99999.99 -99999.99 -99999.99      0.0   \n",
       "Max  19999.9  999.9  ...  99999.99  99999.99  99999.99  99999.99  99999.0   \n",
       "\n",
       "         C34       C37    C39    C40     C47  \n",
       "Min      0.0       0.0   0.00   0.00     0.0  \n",
       "Max  99999.0  999999.0  99.99  99.99  9999.0  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tndf = ndf.transpose()\n",
    "tndf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b8f02e-213d-49d9-a8ed-ac5f9f3af38c",
   "metadata": {},
   "source": [
    "## 3.3. String value sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae9b73d",
   "metadata": {},
   "source": [
    "    [Description]: Assign the nature of HF data entry extraction metod: borehole/mine or probe sensing. Also provide controlled vocabulary for columns that store string values. By controlled vocabulary it means the permissible options stored as values for a given column. It is possible to store multiple values in the same column for a particular entry. The allowed controlled vocabulary is taken from \"Appendix A. Structure and field definitions of the IHFC Global Heat Flow Database\" in the aforementioned paper. Also revised for the newer release 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2d3dc0f-c273-4f78-9bbd-2788a91cd6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "number = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87ecc925-fe12-4686-aa3e-a08d10e2749c",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = [\"[Drilling]\",\"[Drilling-Clustering]\",\"[Mining]\",\"[Tunneling]\",\"[GTM]\"]\n",
    "P = [\"[Probing (onshore/lake, river, etc.)]\",\"[Probing (offshore/ocean)]\",\"[Probing-Clustering]\"]\n",
    "U = [\"[Other (specify in comments)]\",\"[unspecified]\",\"nan\",\"\"];\n",
    "sP7 = [\"[Onshore (continental)]\",\"[Onshore (lake, river, etc.)]\",\"[Offshore (continental)]\",\"[Offshore (marine)]\",\"[unspecified])\"];\n",
    "sP9=sC9 = [\"[Yes]\",\"[No]\",\"[Unspecified]\"];\n",
    "sP12 = [\"[Drilling]\",\"[Mining]\",\"[Tunneling]\",\"[GTM]\",\"[Probing (onshore/lake, river, etc.)]\",\"[Probing (offshore/ocean)]\",\"[Drilling-Clustering]\",\"[Probing-Clustering]\",\"[Other (specify in comments)]\",\"[unspecified]\"];\n",
    "sP13 = [\"[Hydrocarbon]\",\"[Underground storage]\",\"[Geothermal]\",\"[Groundwater]\",\"[Mapping]\",\"[Research]\",\"[Mining]\",\"[Tunneling]\",\"[Other (specify in comments)]\",\"[unspecified]\"];\n",
    "sC3 = [\"[Interval method]\",\"[Bullard method]\",\"[Boot-strapping method]\",\"[Numerical inversion]\",\"[Other (specify in coments)]\",\"[unspecified]\"];\n",
    "sC11 = [\"[Considered – p]\",\"[Considered – T]\",\"[Considered – pT]\",\"[not considered]\",\"[unspecified]\"];\n",
    "sC12 = [\"[Tilt corrected]\",\"[Drift corrected]\",\"[not corrected]\",\"[Corrected (specify)]\",\"[unspecified]\"];\n",
    "sC13=sC14=sC15=sC16=sC17=sC18=sC19 = [\"[Present and corrected]\",\"[Present and not corrected]\",\"[Present not significant]\",\"[not recognized]\",\"[unspecified]\"];\n",
    "sC20 = [\"[Expedition/Cruise number]\",\"[R/V Ship]\",\"[D/V Platform]\",\"[D/V Glomar Challenger]\",\"[D/V JOIDES Resolution]\",\"[Other (specify in comments)]\",\"[unspecified]\"];\n",
    "sC21 = [\"[Single Steel probe (Bullard)]\",\"[Single Steel probe (Bullard) in-situ TC]\",\"[Violin-Bow probe (Lister)]\",\"[Outrigger probe (Von Herzen) in-situ TC, without corer]\",\"[Outrigger probe (Haenel) in-situ TC, with corer]\",\"[Outrigger probe (Ewing) with corer]\",\"[Outrigger probe (Ewing) without corer]\",\"[Outrigger probe (Lister) with corer]\",\"[Outrigger probe (autonomous) without corer]\",\"[Outrigger probe (autonomous) with corer]\",\"[Submersible probe]\",\"[Other (specify in comments)]\",\"[unspecified]\"];\n",
    "sC31 = [\"[LOGeq]\",\"[LOGpert]\",\"[cLOG]\",\"[DTSeq]\",\"[DTSpert]\",\"[cDTS]\",\"[BHT]\",\"[cBHT]\",\"[DST]\",\"[cDST]\",\"[RTDeq]\",\"[RTDpert]\",\"[cRTD]\",\"[CPD]\",\"[XEN]\",\"[GTM]\",\"[BSR]\",\"[BLK]\",\"[ODTT-PC]\",\"[ODTT-TP]\",\"[SUR]\",\"[unspecified]\",\"[Other (specify in comments)]\"];\n",
    "sC32 = [\"[LOGeq]\",\"[LOGpert]\",\"[cLOG]\",\"[DTSeq]\",\"[DTSpert]\",\"[cDTS]\",\"[BHT]\",\"[cBHT]\",\"[DST]\",\"[cDST]\",\"[RTDeq]\",\"[RTDpert]\",\"[cRTD]\",\"[CPD]\",\"[XEN]\",\"[GTM]\",\"[BSR]\",\"[BLK]\",\"[ODTT-PC]\",\"[ODTT-TP]\",\"[unspecified]\",\"[Other (specify in comments)]\"];\n",
    "sC35=sC36 = [\"[Horner plot]\",\"[Cylinder source method]\",\"[Line source explosion method]\",\"[Inverse numerical modelling]\",\"[Other published correction]\",\"[unspecified]\",\"[not corrected]\",\"[AAPG correction]\"];  \n",
    "sC41 = [\"[In-situ probe]\",\"[Core-log integration]\",\"[Core samples]\",\"[Cutting samples]\",\"[Outcrop samples]\",\"[Well-log interpretation]\",\"[Mineral computation]\",\"[Assumed from literature]\",\"[other (specify)]\",\"[unspecified]\"];\n",
    "sC42 = [\"[Actual heat-flow location]\",\"[Other location]\",\"[Literature/unspecified]\",\"[Unspecified]\"];\n",
    "sC43 = [\"[Lab - point source]\",\"[Lab - line source / full space]\",\"[Lab - line source / half space]\",\"[Lab - plane source / full space]\",\"[Lab - plane source / half space]\",\"[Lab - other]\",\"[Probe - pulse technique]\",\"[Well-log - deterministic approach]\",\"[Well-log - empirical equation]\",\"[Estimation - from chlorine content]\",\"[Estimation - from water content/porosity]\",\"[Estimation - from lithology and literature]\",\"[Estimation - from mineral composition]\",\"[unspecified]\"];\n",
    "sC44 = [\"[Saturated measured in-situ]\",\"[Recovered]\",\"[Saturated measured]\",\"[Saturated calculated]\",\"[Dry measured]\",\"[other (specify)]\",\"[unspecified]\"];\n",
    "sC45 = [\"[Unrecorded ambient pT conditions]\",\"[Recorded ambient pT conditions]\",\"[Actual in-situ (pT) conditions]\",\"[Replicated in-situ (p)]\",\"[Replicated in-situ (T)]\",\"[Replicated in-situ (pT)]\",\"[Corrected in-situ (p)]\",\"[Corrected in-situ (T)]\",\"[Corrected in-situ (pT)]\",\"[unspecified]\"];\n",
    "sC46 = [\"[T - Birch and Clark (1940)]\",\"[T - Tikhomirov (1968)]\",\"[T - Kutas & Gordienko (1971)]\",\"[T - Anand et al. (1973)]\",\"[T - Haenel & Zoth (1973)]\",\"[T - Blesch et al. (1983)]\",\"[T - Sekiguchi (1984)]\",\"[T - Chapman et al. (1984)]\",\"[T - Zoth & Haenel (1988)]\",\"[T - Somerton (1992)]\",\"[T - Sass et al. (1992)]\",\"[T - Funnell et al. (1996)]\",\"[T - Kukkonen et al. (1999)]\",\"[T - Seipold (2001)]\",\"[T - Vosteen & Schellschmidt (2003)]\",\"[T - Sun et al. (2017)]\",\"[T - Miranda et al. (2018)]\",\"[T - Ratcliffe (1960)]\",\"[p - Bridgman (1924)]\",\"[p - Sibbitt (1975)]\",\"[p - Kukkonen et al. (1999)]\",\"[p - Seipold (2001)]\",\"[p - Durutürk et al. (2002)]\",\"[p - Demirci et al. (2004)]\",\"[p - Görgülü et al. (2008)]\",\"[p - Fuchs & Förster (2014)]\",\"[pT - Ratcliffe (1960)]\",\"[pT - Buntebarth (1991)]\",\"[pT - Chapman & Furlong (1992)]\",\"[pT - Emirov et al. (1997)]\",\"[pT - Abdulagatov et al. (2006)]\",\"[pT - Emirov & Ramazanova (2007)]\",\"[pT - Abdulagatova et al. (2009)]\",\"[pT - Ramazanova & Emirov (2010)]\",\"[pT - Ramazanova & Emirov (2012)]\",\"[pT - Emirov et al. (2017)]\",\"[pT - Hyndman et al. (1974)]\",\"[Site-specific experimental relationships]\",\"[Other (specify in comments)]\",\"[unspecified]\"];\n",
    "#sC48 = [\"[Random or periodic depth sampling (number)]\",\"[Characterize formation conductivities]\",\"[Well log interpretation]\",\"[Computation from probe sensing]\",\"[Other]\",\"[unspecified]\"];\n",
    "sC48 = [f\"[Random or periodic depth sampling ({number})]\",\"[Characterize formation conductivities]\",\"[Well log interpretation]\",\"[Computation from probe sensing]\",\"[Other]\",\"[unspecified]\"];"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7207f491",
   "metadata": {},
   "source": [
    "    [Description]: To avoid case-sensitivity issues in the controlled vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de517e36-afb9-4724-a96e-273f6de46ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = [item.lower() for item in B]\n",
    "P = [item.lower() for item in P]\n",
    "U = [item.lower() for item in U]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2bdd67",
   "metadata": {},
   "source": [
    "    [Description]: To store the controlled vocabulary in a dataframe structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f91bf08f-26c2-43f9-a829-771fbab2b7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_data = {\n",
    "    #'ID': ['P7','P9','P12','P13','C3','C11','C12','C13','C14','C15','C17','C18','C19','C21','C31','C32','C35','C36','C41','C42','C43','C44','C45','C46','C48'],#,C20\n",
    "    'Values': [sP7,sP9,sP12,sP13,sC3,sC11,sC12,sC13,sC14,sC15,sC17,sC18,sC19,sC21,sC31,sC32,sC35,sC36,sC41,sC42,sC43,sC44,sC45,sC46,sC48],#,sC20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8b8d342-c6b6-42a9-bb7f-4d7145355e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = pd.DataFrame(str_data, index=StrC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e996c8c-1809-4b43-8468-309b9b98a422",
   "metadata": {},
   "source": [
    "### 3.3.1. Pivot the DataFrame: Rows become columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c499ef1a-d2ff-47d4-adc6-0618313405ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P7</th>\n",
       "      <th>P9</th>\n",
       "      <th>P12</th>\n",
       "      <th>P13</th>\n",
       "      <th>C3</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>C15</th>\n",
       "      <th>...</th>\n",
       "      <th>C32</th>\n",
       "      <th>C35</th>\n",
       "      <th>C36</th>\n",
       "      <th>C41</th>\n",
       "      <th>C42</th>\n",
       "      <th>C43</th>\n",
       "      <th>C44</th>\n",
       "      <th>C45</th>\n",
       "      <th>C46</th>\n",
       "      <th>C48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <td>[[Onshore (continental)], [Onshore (lake, rive...</td>\n",
       "      <td>[[Yes], [No], [Unspecified]]</td>\n",
       "      <td>[[Drilling], [Mining], [Tunneling], [GTM], [Pr...</td>\n",
       "      <td>[[Hydrocarbon], [Underground storage], [Geothe...</td>\n",
       "      <td>[[Interval method], [Bullard method], [Boot-st...</td>\n",
       "      <td>[[Considered – p], [Considered – T], [Consider...</td>\n",
       "      <td>[[Tilt corrected], [Drift corrected], [not cor...</td>\n",
       "      <td>[[Present and corrected], [Present and not cor...</td>\n",
       "      <td>[[Present and corrected], [Present and not cor...</td>\n",
       "      <td>[[Present and corrected], [Present and not cor...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[LOGeq], [LOGpert], [cLOG], [DTSeq], [DTSpert...</td>\n",
       "      <td>[[Horner plot], [Cylinder source method], [Lin...</td>\n",
       "      <td>[[Horner plot], [Cylinder source method], [Lin...</td>\n",
       "      <td>[[In-situ probe], [Core-log integration], [Cor...</td>\n",
       "      <td>[[Actual heat-flow location], [Other location]...</td>\n",
       "      <td>[[Lab - point source], [Lab - line source / fu...</td>\n",
       "      <td>[[Saturated measured in-situ], [Recovered], [S...</td>\n",
       "      <td>[[Unrecorded ambient pT conditions], [Recorded...</td>\n",
       "      <td>[[T - Birch and Clark (1940)], [T - Tikhomirov...</td>\n",
       "      <td>[[Random or periodic depth sampling (0)], [Cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       P7  \\\n",
       "Values  [[Onshore (continental)], [Onshore (lake, rive...   \n",
       "\n",
       "                                  P9  \\\n",
       "Values  [[Yes], [No], [Unspecified]]   \n",
       "\n",
       "                                                      P12  \\\n",
       "Values  [[Drilling], [Mining], [Tunneling], [GTM], [Pr...   \n",
       "\n",
       "                                                      P13  \\\n",
       "Values  [[Hydrocarbon], [Underground storage], [Geothe...   \n",
       "\n",
       "                                                       C3  \\\n",
       "Values  [[Interval method], [Bullard method], [Boot-st...   \n",
       "\n",
       "                                                      C11  \\\n",
       "Values  [[Considered – p], [Considered – T], [Consider...   \n",
       "\n",
       "                                                      C12  \\\n",
       "Values  [[Tilt corrected], [Drift corrected], [not cor...   \n",
       "\n",
       "                                                      C13  \\\n",
       "Values  [[Present and corrected], [Present and not cor...   \n",
       "\n",
       "                                                      C14  \\\n",
       "Values  [[Present and corrected], [Present and not cor...   \n",
       "\n",
       "                                                      C15  ...  \\\n",
       "Values  [[Present and corrected], [Present and not cor...  ...   \n",
       "\n",
       "                                                      C32  \\\n",
       "Values  [[LOGeq], [LOGpert], [cLOG], [DTSeq], [DTSpert...   \n",
       "\n",
       "                                                      C35  \\\n",
       "Values  [[Horner plot], [Cylinder source method], [Lin...   \n",
       "\n",
       "                                                      C36  \\\n",
       "Values  [[Horner plot], [Cylinder source method], [Lin...   \n",
       "\n",
       "                                                      C41  \\\n",
       "Values  [[In-situ probe], [Core-log integration], [Cor...   \n",
       "\n",
       "                                                      C42  \\\n",
       "Values  [[Actual heat-flow location], [Other location]...   \n",
       "\n",
       "                                                      C43  \\\n",
       "Values  [[Lab - point source], [Lab - line source / fu...   \n",
       "\n",
       "                                                      C44  \\\n",
       "Values  [[Saturated measured in-situ], [Recovered], [S...   \n",
       "\n",
       "                                                      C45  \\\n",
       "Values  [[Unrecorded ambient pT conditions], [Recorded...   \n",
       "\n",
       "                                                      C46  \\\n",
       "Values  [[T - Birch and Clark (1940)], [T - Tikhomirov...   \n",
       "\n",
       "                                                      C48  \n",
       "Values  [[Random or periodic depth sampling (0)], [Cha...  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsdf = sdf.transpose()\n",
    "tsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7defc83-bd46-4558-951d-5f19f87fe999",
   "metadata": {},
   "source": [
    "## 3.4. Case sensitivity issue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b02c03d",
   "metadata": {},
   "source": [
    "    [Description]: To avoid case-sensitivity issues in the controlled vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b687305e-1f3a-4c61-aec3-60fa1102f906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P7</th>\n",
       "      <th>P9</th>\n",
       "      <th>P12</th>\n",
       "      <th>P13</th>\n",
       "      <th>C3</th>\n",
       "      <th>C11</th>\n",
       "      <th>C12</th>\n",
       "      <th>C13</th>\n",
       "      <th>C14</th>\n",
       "      <th>C15</th>\n",
       "      <th>...</th>\n",
       "      <th>C32</th>\n",
       "      <th>C35</th>\n",
       "      <th>C36</th>\n",
       "      <th>C41</th>\n",
       "      <th>C42</th>\n",
       "      <th>C43</th>\n",
       "      <th>C44</th>\n",
       "      <th>C45</th>\n",
       "      <th>C46</th>\n",
       "      <th>C48</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Values</th>\n",
       "      <td>[[onshore (continental)], [onshore (lake, rive...</td>\n",
       "      <td>[[yes], [no], [unspecified]]</td>\n",
       "      <td>[[drilling], [mining], [tunneling], [gtm], [pr...</td>\n",
       "      <td>[[hydrocarbon], [underground storage], [geothe...</td>\n",
       "      <td>[[interval method], [bullard method], [boot-st...</td>\n",
       "      <td>[[considered – p], [considered – t], [consider...</td>\n",
       "      <td>[[tilt corrected], [drift corrected], [not cor...</td>\n",
       "      <td>[[present and corrected], [present and not cor...</td>\n",
       "      <td>[[present and corrected], [present and not cor...</td>\n",
       "      <td>[[present and corrected], [present and not cor...</td>\n",
       "      <td>...</td>\n",
       "      <td>[[logeq], [logpert], [clog], [dtseq], [dtspert...</td>\n",
       "      <td>[[horner plot], [cylinder source method], [lin...</td>\n",
       "      <td>[[horner plot], [cylinder source method], [lin...</td>\n",
       "      <td>[[in-situ probe], [core-log integration], [cor...</td>\n",
       "      <td>[[actual heat-flow location], [other location]...</td>\n",
       "      <td>[[lab - point source], [lab - line source / fu...</td>\n",
       "      <td>[[saturated measured in-situ], [recovered], [s...</td>\n",
       "      <td>[[unrecorded ambient pt conditions], [recorded...</td>\n",
       "      <td>[[t - birch and clark (1940)], [t - tikhomirov...</td>\n",
       "      <td>[[random or periodic depth sampling (0)], [cha...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                       P7  \\\n",
       "Values  [[onshore (continental)], [onshore (lake, rive...   \n",
       "\n",
       "                                  P9  \\\n",
       "Values  [[yes], [no], [unspecified]]   \n",
       "\n",
       "                                                      P12  \\\n",
       "Values  [[drilling], [mining], [tunneling], [gtm], [pr...   \n",
       "\n",
       "                                                      P13  \\\n",
       "Values  [[hydrocarbon], [underground storage], [geothe...   \n",
       "\n",
       "                                                       C3  \\\n",
       "Values  [[interval method], [bullard method], [boot-st...   \n",
       "\n",
       "                                                      C11  \\\n",
       "Values  [[considered – p], [considered – t], [consider...   \n",
       "\n",
       "                                                      C12  \\\n",
       "Values  [[tilt corrected], [drift corrected], [not cor...   \n",
       "\n",
       "                                                      C13  \\\n",
       "Values  [[present and corrected], [present and not cor...   \n",
       "\n",
       "                                                      C14  \\\n",
       "Values  [[present and corrected], [present and not cor...   \n",
       "\n",
       "                                                      C15  ...  \\\n",
       "Values  [[present and corrected], [present and not cor...  ...   \n",
       "\n",
       "                                                      C32  \\\n",
       "Values  [[logeq], [logpert], [clog], [dtseq], [dtspert...   \n",
       "\n",
       "                                                      C35  \\\n",
       "Values  [[horner plot], [cylinder source method], [lin...   \n",
       "\n",
       "                                                      C36  \\\n",
       "Values  [[horner plot], [cylinder source method], [lin...   \n",
       "\n",
       "                                                      C41  \\\n",
       "Values  [[in-situ probe], [core-log integration], [cor...   \n",
       "\n",
       "                                                      C42  \\\n",
       "Values  [[actual heat-flow location], [other location]...   \n",
       "\n",
       "                                                      C43  \\\n",
       "Values  [[lab - point source], [lab - line source / fu...   \n",
       "\n",
       "                                                      C44  \\\n",
       "Values  [[saturated measured in-situ], [recovered], [s...   \n",
       "\n",
       "                                                      C45  \\\n",
       "Values  [[unrecorded ambient pt conditions], [recorded...   \n",
       "\n",
       "                                                      C46  \\\n",
       "Values  [[t - birch and clark (1940)], [t - tikhomirov...   \n",
       "\n",
       "                                                      C48  \n",
       "Values  [[random or periodic depth sampling (0)], [cha...  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tsdf = tsdf\n",
    "for col in tsdf.columns:\n",
    "    for id in tsdf.index:\n",
    "        if isinstance(tsdf.loc[id, col], list):\n",
    "            tsdf.loc[id, col] = [str(item).lower() for item in tsdf.loc[id, col]]\n",
    "tsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a14f962-95e1-4e94-89c7-1dfdd5c3661c",
   "metadata": {},
   "source": [
    "# 4. Remove extra rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75f43b",
   "metadata": {},
   "source": [
    "    [Description]: To perform computations on the entered HF entries only and skip the column labels. There are two conditions: firstly, when the first cell of the dataframe has the column label 'Obligation', the top 8 rows are considered description. Secondly, when the first cell has the column label 'Short Name', the top 2 rows are considered description. The function 'remove_rows()' below switches between these two conditions and removes the description to prepare the dataframe for operability with other functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "db173255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rows(df):\n",
    "    if df.at[0,'ID'] == 'Obligation':\n",
    "        df_copy = df\n",
    "        top_rows = df_copy.index[0:7]\n",
    "        df_copy = df_copy.drop(df_copy.index[0:7])\n",
    "\n",
    "        new_index_values = range(1, 1+len(df_copy))\n",
    "        df_copy.index = new_index_values\n",
    "        #flag = 6\n",
    "        return df_copy\n",
    "    elif df.at[0,'ID'] == 'Short Name':\n",
    "        df_copy = df\n",
    "        top_rows = df_copy.index[0:1]\n",
    "        df_copy = df_copy.drop(df_copy.index[0:1])\n",
    "\n",
    "        new_index_values = range(1, 1+len(df_copy))\n",
    "        df_copy.index = new_index_values\n",
    "        #flag = 1\n",
    "        return df_copy\n",
    "    else:\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba2d33-7fb4-4923-a4ae-b21d531144d0",
   "metadata": {},
   "source": [
    "# 5. Data type handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf1f913-5016-44ec-ae4c-1167536ff823",
   "metadata": {},
   "source": [
    "## 5.1. Assigning data types to specific columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5608178",
   "metadata": {},
   "source": [
    "    [Description]: Convert all the columns to string data type. To resolve multiple values in a categorical field for an entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7355f55e-e4fd-4663-bb6a-704836912581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_type(df):\n",
    "    df[NumC] = df[NumC].astype(str)\n",
    "    df[StrC] = df[StrC].astype(str)\n",
    "    df[DateC] = df[DateC].astype(str)   \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0636775f-5c95-4e6f-be58-501de18844a0",
   "metadata": {},
   "source": [
    "## 5.2. Safe float conversion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7351cb74-e854-4d1c-864b-3298667953f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_float_conversion(r):\n",
    "    r = r.strip()  # Remove any leading or trailing whitespace    \n",
    "    if r == '0':\n",
    "        return 0.0    \n",
    "    try:\n",
    "        # Check if the first character is a minus sign\n",
    "        if r[0] == '-':\n",
    "            return -float(r[1:])  # Convert the substring starting from the second character to float and make it negative\n",
    "        else:\n",
    "            return float(r)  # Convert the whole string to float\n",
    "    except ValueError:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55399ce3-288e-4dc0-b7e5-bd9b239010af",
   "metadata": {},
   "source": [
    "# 6. Converting string values to lower case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945796f",
   "metadata": {},
   "source": [
    "    [Description]: To resolve case-sensitivity in the provided Heatflow database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fee3591-46d2-4279-92af-e506d90a84dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def toLower(df):\n",
    "    for col in tsdf.columns:\n",
    "        for id in df.index:\n",
    "            df.loc[id, col] = (df.loc[id, col]).lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1f877e-c6eb-41fd-bf08-11da8e416595",
   "metadata": {},
   "source": [
    "# 7. Check relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f4e3a",
   "metadata": {},
   "source": [
    "## 7.1. Obligation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7161171",
   "metadata": {},
   "source": [
    "    [Description]: Check for mandatory fields indicated by 'Obligation' label in HF database. And store information about the nature of data, whether its borehole or probe sensing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8570b129-5df2-4764-b1fc-cc42f33e8003",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obligation(df):\n",
    "    m_dict = {}\n",
    "    domain = {}\n",
    "    for c in df:\n",
    "        m_dict[c] = df.loc[0, c]\n",
    "        domain[c] = df.loc[1, c]\n",
    "    return m_dict, domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8310b279",
   "metadata": {},
   "source": [
    "## 7.2.  Structure relevance for the current release"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc9c47a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relevance(folder_path):\n",
    "    files = os.listdir(folder_path)\n",
    "    csv_files = [file for file in files if file.endswith('.csv')]\n",
    "\n",
    "    if csv_files:\n",
    "        first_csv_file_path = os.path.join(folder_path, csv_files[0])\n",
    "        df = pd.read_csv(first_csv_file_path)\n",
    "        m_dict, domain = obligation(df)\n",
    "    else:\n",
    "        print(\"No CSV files found in the directory. Please run 'convert2UTF8csv(folder_path)' function\")\n",
    "    return m_dict, domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ea4dbc-21cc-47fb-b4d3-56156f4e0bfc",
   "metadata": {},
   "source": [
    "# 8. Vocabulary check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb9c7d3",
   "metadata": {},
   "source": [
    "    [Description]: Complete check of vocabulary separately for numeric, string and date type columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa1628c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[probing-clustering]', '[probing (offshore/ocean)]']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fg = \"[unspecified];[probing (offshore/ocean)];nan;\"\n",
    "#fg = \"[drilling-clustering];[probing (offshore/ocean)]\"\n",
    "#fg = ''\n",
    "fg = '[probing-clustering];[probing (offshore/ocean)]'\n",
    "fg_split = fg.split(';')\n",
    "fg_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cfb26ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if any(value in ['[other (specify in comments)]', '[unspecified]'] for value in fg_split):\n",
    "    error_string = \" P12:Quality Check is not possible!,\"\n",
    "elif any((value == 'nan' or '') for value in fg_split):\n",
    "    error_string = \" P12:Mandatory entry is empty; Quality Check is not possible!,\"\n",
    "elif any(value in P for value in fg_split) and any(value in B for value in fg_split):\n",
    "    error_string = \" P12:Quality Check is not possible!,\"\n",
    "else:\n",
    "    error_string = \"\"\n",
    "error_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ae3db32d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[probing-clustering]'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if any(value in U for value in fg_split):\n",
    "    P12 = \"\"\n",
    "elif any(value in P for value in fg_split) and any(value in B for value in fg_split):\n",
    "    P12 = \"\"\n",
    "else:\n",
    "    P12 = fg_split[0] if fg_split else fg_split\n",
    "P12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67c3e7b8-9945-4c9b-a8e6-3a663868f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocabcheck(df,m_dict,domain):\n",
    "    error_df = pd.DataFrame()\n",
    "    error_msg =pd.DataFrame()\n",
    "    error_msg_counter = 0\n",
    "\n",
    "    for id in df.index:\n",
    "        error_df.loc[id,'A'] = None\n",
    "        error_df['A'] = error_df['A'].astype(\"string\")\n",
    "\n",
    "        P12_split = (df.loc[id, 'P12']).split(';')\n",
    "        \n",
    "        if any(value in ['[other (specify in comments)]', '[unspecified]'] for value in P12_split):\n",
    "            error_string = \" P12:Quality Check is not possible!,\"\n",
    "        elif any(value == 'nan' for value in P12_split):\n",
    "            error_string = \" P12:Mandatory entry is empty; Quality Check is not possible!,\"\n",
    "        elif any(value in P for value in P12_split) and any(value in B for value in P12_split):\n",
    "            error_string = \" P12:Quality Check is not possible!,\"\n",
    "        else:\n",
    "            error_string = \"\"\n",
    "\n",
    "        error_df.loc[id,'A'] = error_string\n",
    "        \n",
    "\n",
    "    for c in NumC:\n",
    "        min_value = tndf.loc['Min', c]\n",
    "        max_value = tndf.loc['Max', c]\n",
    "        \n",
    "        for id in df.index:\n",
    "            error_df.loc[id,c] = None\n",
    "            error_df[c] = error_df[c].astype(\"string\")\n",
    "            dfvalue = df.loc[id,c]\n",
    "\n",
    "            P12_split = (df.loc[id, 'P12']).split(';')\n",
    "            if any(value in U for value in P12_split):\n",
    "                P12 = \"\"\n",
    "            elif any(value in P for value in P12_split) and any(value in B for value in P12_split):\n",
    "                P12 = \"\"\n",
    "            else:\n",
    "                P12 = P12_split[0] if P12_split else df.loc[id, 'P12']\n",
    "\n",
    "            while True:\n",
    "                dfvalue = dfvalue.split(';')\n",
    "                \n",
    "                for dfvalue in dfvalue:\n",
    "                    try:\n",
    "                        r = dfvalue.strip()\n",
    "                        if float(r) or r=='0':\n",
    "                            r = safe_float_conversion(r)\n",
    "    \n",
    "                            if  min_value <= r <= max_value:\n",
    "                                error_string = \"\"\n",
    "                                \n",
    "                            elif math.isnan(r):\n",
    "                                if (m_dict[c] == 'M') and (df.loc[id, c]) == 'nan':\n",
    "                                    if ('B' in domain[c] and (P12 in B)):\n",
    "                                        error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                                    elif ('S' in domain[c] and (P12 in P)):                                        \n",
    "                                        if (c == 'C4') and (df.loc[id, 'P6']) != 'nan':\n",
    "                                            error_string = \"\"\n",
    "                                        else:\n",
    "                                            error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                                            \n",
    "                                    elif ((('B'or'S') in domain[c]) and (P12 in U)):\n",
    "                                        error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                                    else:\n",
    "                                        error_string = \"\"\n",
    "                                elif m_dict[c] == 'M':\n",
    "                                    if P12 in B:\n",
    "                                        if (c == 'C5') and (df.loc[id, 'C6'] is None):\n",
    "                                            error_string = f\" {c}:mandatory field!,\"   \n",
    "                                        else:\n",
    "                                            error_string = \"\"\n",
    "                                    elif P12 in P:\n",
    "                                        if (c == 'C6') and (df.loc[id, 'C5'] is None):\n",
    "                                            error_string = f\" {c}:mandatory field!,\"\n",
    "                                        elif (c == 'C23') and ((df.loc[id, 'C31'] or df.loc[id, 'C32']) is None):\n",
    "                                            error_string = f\" {c}:mandatory field!,\"\n",
    "                                        else:\n",
    "                                            error_string = \"\"\n",
    "                                else:\n",
    "                                    error_string = \"\"\n",
    "                            else:\n",
    "                                error_string = f\" {c}:range violated,\" ###                                                                      \n",
    "                        else:\n",
    "                            error_string = f\" {c}:range violated,\"\n",
    "                    except ValueError:\n",
    "                            error_string = f\" {c}:invalid format,\" \n",
    "                        \n",
    "                    error_df.loc[id,c] = error_string\n",
    "                    if error_string != \"\":\n",
    "                        error_msg_counter= error_msg_counter+1\n",
    "\n",
    "                if ';' not in dfvalue:\n",
    "                    break\n",
    "                else:\n",
    "                    dfvalue = dfvalue[-1]\n",
    "                         \n",
    "    for c in StrC:\n",
    "        string_values = tsdf.loc['Values', c]\n",
    "\n",
    "        for id in df.index:\n",
    "            error_df.loc[id,c] = None\n",
    "            error_df[c] = error_df[c].astype(\"string\")\n",
    "            dfvalue = df.loc[id,c]\n",
    "\n",
    "            P12_split = (df.loc[id, 'P12']).split(';')\n",
    "\n",
    "            if any(value in U for value in P12_split):\n",
    "                P12 = \"\"\n",
    "            elif any(value in P for value in P12_split) and any(value in B for value in P12_split):\n",
    "                P12 = \"\"\n",
    "            else:\n",
    "                P12 = P12_split[0] if P12_split else df.loc[id, 'P12']\n",
    "\n",
    "            while True:\n",
    "                dfvalue = dfvalue.split(';')\n",
    "\n",
    "                for dfvalue in dfvalue:\n",
    "                    dfvalue = dfvalue.strip()\n",
    "                    \n",
    "                    #new modifications\n",
    "                    if (c == 'C48') and (dfvalue.startswith(\"[random or periodic depth sampling (\")):\n",
    "                        start_idx = dfvalue.find('(')\n",
    "                        end_idx = dfvalue.find(')')\n",
    "                        number_str = dfvalue[start_idx + 1:end_idx]\n",
    "                        \n",
    "                        try:\n",
    "                            number = int(number_str)\n",
    "                            string_values[0] = f\"[random or periodic depth sampling ({number})]\"\n",
    "                                                                                 \n",
    "                            if dfvalue in string_values:\n",
    "                                error_string = \"\"\n",
    "                            else:\n",
    "                                error_string = f\" {c}:vocabulary warning,\"\n",
    "                            \n",
    "                        except ValueError: \n",
    "                            error_string = f\" {c}:vocabulary warning,\"\n",
    "                            \n",
    "\n",
    "                    elif dfvalue in string_values:\n",
    "                        error_string = \"\"\n",
    "        \n",
    "                    elif dfvalue == 'nan':\n",
    "                        if m_dict[c] == 'M':\n",
    "                            if (c == 'C31' or 'C32') and (df.loc[id, 'C23'] is None):\n",
    "                                error_string = f\" {c}:mandatory field!,\"\n",
    "                            else:\n",
    "                                if ('B' in domain[c] and (P12 in B)):\n",
    "                                    error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                                elif ('S' in domain[c] and (P12 in P)):\n",
    "                                    error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                                elif ((('B'or'S') in domain[c]) and (P12 in U)):\n",
    "                                    error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                                else:\n",
    "                                    error_string = \"\" #pass\n",
    "                        else:\n",
    "                            error_string = \"\"       \n",
    "                    else:\n",
    "                        error_string = f\" {c}:vocabulary warning,\"\n",
    "        \n",
    "                    error_df.loc[id,c] = error_string\n",
    "                    if error_string != \"\":\n",
    "                        error_msg_counter= error_msg_counter+1\n",
    "                        \n",
    "                if ';' not in dfvalue:\n",
    "                    break\n",
    "                else:\n",
    "                    dfvalue = dfvalue[-1]\n",
    "    \n",
    "    # Compare the input date with January 1900\n",
    "    jan_1900 = datetime(1900, 1, 1)\n",
    "    for id in df.index:\n",
    "        error_df.loc[id,'C38'] = None\n",
    "        error_df['C38'] = error_df['C38'].astype(\"string\")\n",
    "        dfvalue = df.loc[id,'C38']\n",
    "\n",
    "        P12_split = (df.loc[id, 'P12']).split(';')\n",
    "\n",
    "        if any(value in U for value in P12_split):\n",
    "            P12 = \"\"\n",
    "        elif any(value in P for value in P12_split) and any(value in B for value in P12_split):\n",
    "            P12 = \"\"\n",
    "        else:\n",
    "            P12 = P12_split[0] if P12_split else df.loc[id, 'P12']\n",
    "\n",
    "        while True:\n",
    "                dfvalue = dfvalue.split(';')\n",
    "                    \n",
    "                for dfvalue in dfvalue:\n",
    "                    dfvalue = dfvalue.strip()\n",
    "                    \n",
    "                    if dfvalue == '[unspecified]':\n",
    "                        error_string = \"\"\n",
    "                    elif df.loc[id, 'C38'] == 'nan':\n",
    "                        if ('B' in domain[c] and (P12 in B)):\n",
    "                            error_string = \" C38:Mandatory entry is empty!,\"\n",
    "                        elif ('S' in domain[c] and (P12 in P)):\n",
    "                            error_string = \" C38:Mandatory entry is empty!,\"\n",
    "                        elif ((('B'or'S') in domain[c]) and (P12 in U)):\n",
    "                            error_string = f\" {c}:Mandatory entry is empty!,\"\n",
    "                        else:\n",
    "                            error_string = \"\" #pass\n",
    "                    else:                        \n",
    "                        try:\n",
    "                            if dfvalue[-2:] == \"99\":\n",
    "                                year = int(dfvalue[:4])\n",
    "                                input_date = datetime(year, 1, 1)\n",
    "                            else:\n",
    "                                input_date = datetime.strptime(dfvalue, '%Y-%m')\n",
    "                            \n",
    "                            if input_date.month == 1 and input_date.year >= jan_1900.year:\n",
    "                                error_string = \"\"\n",
    "                            elif input_date >= jan_1900:\n",
    "                                error_string = \"\"\n",
    "                            else:\n",
    "                                error_string = \" C38:range violated\"\n",
    "                        except ValueError:\n",
    "                            error_string = f\" C38:invalid format,\"\n",
    "                    if error_string != \"\":\n",
    "                            error_msg_counter= error_msg_counter+1\n",
    "            \n",
    "                    error_df.loc[id,'C38'] = error_string\n",
    "                error_df = error_df.astype(\"string\")\n",
    "                    \n",
    "                if ';' not in dfvalue:\n",
    "                    break\n",
    "                else:\n",
    "                    dfvalue = dfvalue[-1]\n",
    "        \n",
    "    result = error_df.apply(lambda x: ''.join(x), axis=1)\n",
    "    result = result.astype(\"string\")\n",
    "    \n",
    "    error_msg['Error'] = result\n",
    "\n",
    "    return error_msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f3539b-7a4e-4cbf-b859-f7620bdcc056",
   "metadata": {},
   "source": [
    "# 9. Final check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4ff9df-f214-4483-ae0f-45cb62d7533c",
   "metadata": {},
   "source": [
    "## 9.1 Sort error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9a4eb7d-7a54-4500-afa4-5a5ebd803360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reorder_errors(error_str):\n",
    "    #errors = error_str.split(', ')\n",
    "    errors = re.split(r',\\s*', error_str.strip())\n",
    "    \n",
    "    p_errors = [e for e in errors if e.startswith('P')]\n",
    "    c_errors = [e for e in errors if e.startswith('C')]\n",
    "    \n",
    "    p_errors_sorted = sorted(p_errors, key=lambda x: int(x[1:x.index(':')]))\n",
    "    c_errors_sorted = sorted(c_errors, key=lambda x: int(x[1:x.index(':')]))\n",
    "    \n",
    "    sorted_errors = p_errors_sorted + c_errors_sorted\n",
    "    \n",
    "    sorted_errors_str = ', '.join(sorted_errors)\n",
    "    \n",
    "    cleaned_errors_str = re.sub(r',\\s*,\\s*', ', ', sorted_errors_str)\n",
    "\n",
    "    if cleaned_errors_str.endswith(','):\n",
    "        cleaned_errors_str = cleaned_errors_str[:-1]\n",
    "    \n",
    "    return cleaned_errors_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8cf941-3ec2-49e0-958e-3a3460d7469f",
   "metadata": {},
   "source": [
    "## 9.2 Complete check"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac21daba",
   "metadata": {},
   "source": [
    "    [Description]: Calling previous functions to prepare data and perform vocabulary checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88245463-65de-4a0c-a492-c56a44dedd43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Complete_check(df):\n",
    "    m_dict, domain = obligation(df)\n",
    "    result = vocabcheck(toLower(change_type(remove_rows(df))), m_dict, domain)\n",
    "    result['Error'] = result['Error'].apply(reorder_errors)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e790ae",
   "metadata": {},
   "source": [
    "# 10 Attach to original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc3e4a8",
   "metadata": {},
   "source": [
    "    [Description]: Attaching the combined results column to the original database with correct indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42189f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attachOG(og):\n",
    "    result = Complete_check(og)\n",
    "    if og.at[0, 'ID'] == 'Obligation':\n",
    "\n",
    "        result.index = result.index + 6\n",
    "    elif og.at[0, 'ID'] == 'Short Name':\n",
    "\n",
    "        result.index = result.index + 1\n",
    "    \n",
    "    og = pd.merge(og, result[['Error']], left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    return og"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ece2c9",
   "metadata": {},
   "source": [
    "# 11. Result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a83bd8b-3894-4949-b271-cc518b3e13c2",
   "metadata": {},
   "source": [
    "## 11.1 Results of all files in a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c85af3",
   "metadata": {},
   "source": [
    "    [Description]: To generate results for all the Heatflow database in a folder stored in .csv format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0751545f-c777-4af2-8feb-a46e37a5a581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_result(folder_path):\n",
    "\n",
    "    csv_files = glob.glob(os.path.join(folder_path, '*.csv'))    \n",
    "\n",
    "    for csv_file_path in csv_files:\n",
    "\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        df_result = attachOG(df)\n",
    "\n",
    "        if df_result['Error'].eq('').all():\n",
    "            print(\"There is no error. Data is ready for Quality Check!\")\n",
    "        else:\n",
    "            output_excel_file = os.path.splitext(csv_file_path)[0] + '_vocab_check.xlsx'        \n",
    "            df_result.to_excel(output_excel_file, index=False)\n",
    "            print(f\"Result exported: {output_excel_file}\")\n",
    "\n",
    "    for csv_file_path in csv_files:\n",
    "        os.remove(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d52e1a",
   "metadata": {},
   "source": [
    "# 12. hfqa_tool function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352b11f9",
   "metadata": {},
   "source": [
    "     [Description]: To check the vocabulary for all the HF dataframe files in a folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d34098e",
   "metadata": {},
   "source": [
    "     [Desclaimer]: When a new data release occurs and the relevancy (indicated by 'Obligation') of a column in the HF data structure is updated, ensure that you place the data structure files with the updated column relevancy into separate folders before running the code!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4c75636-5942-40d6-bda8-f0c0e0873878",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vocabulary():\n",
    "    folder_path = input(\"Please enter the file directory: \")\n",
    "    convert2UTF8csv(folder_path)\n",
    "    folder_result(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9d2e4bb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please enter the file directory: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Beardsmore_Cull_1993_SE_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Cull_1982_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Cull_1991_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Gallagher_1990_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Goutorbe_etal._2008a_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Grim_1969_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Halunen_VonHerzen_1973_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Howard_Sass_1964_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Hyndman_1967_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Hyndman_Sass_1966_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Lilley_etal._1977_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Macdonald_etal._1973_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Munroe_etal._1975_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Neumann_etal._2000_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Sass_etal._1976b_vocab_check.xlsx\n",
      "Result exported: Z:\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_Samah\\Samah_To_check\\Beardsmore_Cull_1993_SE\\Swift_1990_vocab_check.xlsx\n",
      "CPU times: total: 31.3 s\n",
      "Wall time: 35.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "check_vocabulary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
