{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3ec036",
   "metadata": {},
   "source": [
    "    Author: Saman Firdaus Chishti   |   chishti@gfz-potsdam.de\n",
    "\n",
    "    Start date: 12-08-2023\n",
    "    \n",
    "**Description:** This code has been developed to assess the quality of the Heatflow database in terms of U-score (Uncertainty quantification), M-Score (Methodological quality), and P-Flags (Perturbation effects). This is in compliance with the paper by Fuchs et al. (2023) titled \"[Quality-assurance of heat-flow data: The new structure and evaluation scheme of the IHFC Global Heat Flow Database](https://doi.org/10.1016/j.tecto.2023.229976),\" published in Tectonophysics 863: 229976. Also revised for the newer release 2024.\n",
    "\n",
    "The code is intended to be published  for the global scientific community to check the quality of any Heatflow dataset, adhering to the data structure described in the aforementioned scientific paper."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1ec26",
   "metadata": {},
   "source": [
    "# 1. Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e7df0f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 125 ms\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "from hfqa_tool.utils.utils import (\n",
    "    readable,\n",
    "    remove_head,\n",
    "    assign_columns,\n",
    "    assign_values\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eaa5398b-1ff2-4634-b30e-0553a393a732",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module='openpyxl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c657c5-8949-4c42-a3ba-47cfd1b0e3be",
   "metadata": {},
   "source": [
    "# 2. Assign Datatype and handle case sensitivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8017d42b",
   "metadata": {},
   "source": [
    "## 2.1. Assigning columns with similar data types to specific list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f974f947",
   "metadata": {},
   "outputs": [],
   "source": [
    "NumC, StrC, DateC = assign_columns()\n",
    "NumC = [col for col in NumC if col not in ['P1', 'P2']]\n",
    "index_C1 = NumC.index('C1')\n",
    "NumC.insert(index_C1 + 1, 'C2')\n",
    "index_C15 = StrC.index('C15')\n",
    "StrC.insert(index_C15 + 1, 'C16')\n",
    "index_C19 = StrC.index('C19')\n",
    "StrC.insert(index_C19 + 1, 'C20')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369fdd0e",
   "metadata": {},
   "source": [
    "## 2.2. Check domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd8d0e9",
   "metadata": {},
   "source": [
    "    [Description]: To check whether the type of HF data entry is of borehole/mine or probe sensing nature. Which is essential to Methodological quality evaluation (M-Score calculation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5fb108b",
   "metadata": {},
   "outputs": [],
   "source": [
    "B, P, U = assign_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1655bd",
   "metadata": {},
   "source": [
    "## 2.3. Assigning data types to specific columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49272c8c",
   "metadata": {},
   "source": [
    "    Description: To convert specified columns to float data type for computation and string columns to lower case to remove case-sensitivy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a069cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_type(df):\n",
    "    df1 = df\n",
    "\n",
    "    for col in NumC:\n",
    "        for index, value in df1[col].items():\n",
    "            try:\n",
    "                float_value = float(value)\n",
    "                df1.at[index, col] = float_value\n",
    "            except (ValueError, TypeError):\n",
    "                df1.at[index, col] = np.nan\n",
    "\n",
    "    df1[StrC] = df1[StrC].astype(str)\n",
    "    for col in StrC:\n",
    "        for id in df1.index:\n",
    "            df1.loc[id, col] = (df1.loc[id, col]).lower()\n",
    "    return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e416feea",
   "metadata": {},
   "source": [
    "# 3. Calculating U score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157404e7",
   "metadata": {},
   "source": [
    "    [Description]: Uncertainty quantification using child Heat-flow values. Determining U-Score by ranging the relative coefficient of variation (COV): estimate by numerical quantification of the heat-flow uncertainty. To avoid divide-by-zero error, such entries are masked out."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e42e67",
   "metadata": {},
   "source": [
    "$$\n",
    "COV(\\%) = \\frac{HFD_{unc}}{HFD_{mean}},\n",
    "$$\n",
    "\n",
    "where, *HFDunc* is the uncertainty of the mean heat-flow density (*HFDmean*) defined as the arithmetic average HFD value (in mW/m2). *HFDunc* is calculated from the error propagation of the uncertainties of the conductivity (Î» in W/mK) and temperature gradient ($\\Delta$*T*  / $\\Delta$*z* in K/m) implemented in the heat-flow calculation (Taylor, 1997):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7006b79a",
   "metadata": {},
   "source": [
    "$$\n",
    "HFD_{unc} = \\sqrt{\\left( \\lambda_{mean} \\cdot \\frac{\\partial T}{\\partial z_{unc}} \\right)^2 + \\left( \\frac{\\partial T}{\\partial z_{mean}} \\cdot \\lambda_{unc} \\right)^2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba5fbdcd-21ce-4954-bcbc-6241293c8d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_U_score(df):\n",
    "    \n",
    "    HFDunc = df['C2'].abs()\n",
    "    HFDmean = df['C1'].abs()\n",
    "\n",
    "    non_zero_mask = (HFDmean != 0) & (~pd.isna(HFDmean)) & (~pd.isna(HFDunc))\n",
    "\n",
    "    COV_prcnt = np.full_like(HFDmean, np.nan)\n",
    "    COV_prcnt[non_zero_mask] = (HFDunc[non_zero_mask] / HFDmean[non_zero_mask]) * 100\n",
    "    \n",
    "    COV = pd.DataFrame(COV_prcnt)\n",
    "    COV.columns = ['COV_percent']\n",
    "    COV['U_score'] = ''\n",
    "    COV['Rank'] = ''\n",
    "\n",
    "    for id in COV.index:\n",
    "        if np.isnan(COV.loc[id,'COV_percent']):\n",
    "            COV.loc[id,'U_score'] = 'Ux'\n",
    "            COV.loc[id,'Rank'] = 'not determined / missing data'\n",
    "            \n",
    "        elif COV.loc[id,'COV_percent'] < 5:\n",
    "            COV.loc[id,'U_score'] = 'U1'\n",
    "            COV.loc[id,'Rank'] = 'Excellent'\n",
    "            \n",
    "        elif 5 <= COV.loc[id,'COV_percent'] <= 15:\n",
    "            COV.loc[id,'U_score'] = 'U2'\n",
    "            COV.loc[id,'Rank'] = 'Good'\n",
    "            \n",
    "        elif 15 < COV.loc[id,'COV_percent'] <= 25:\n",
    "            COV.loc[id,'U_score'] = 'U3'\n",
    "            COV.loc[id,'Rank'] = 'Ok'\n",
    "            \n",
    "        elif COV.loc[id,'COV_percent'] > 25:\n",
    "            COV.loc[id,'U_score'] = 'U4'\n",
    "            COV.loc[id,'Rank'] = 'Poor'\n",
    "        else:\n",
    "            COV.loc[id,'U_score'] = 'Ux'\n",
    "            COV.loc[id,'Rank'] = 'not determined / missing data'\n",
    "\n",
    "    COV.index = COV.index + 1\n",
    "    return COV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0312af",
   "metadata": {},
   "source": [
    "    [Description]: Calling previous functions to prepare data and perform score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4154a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CompleteUscore_calc(df):\n",
    "    result = calc_U_score(change_type(remove_head(df)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc833fa",
   "metadata": {},
   "source": [
    "# 3. Calculating T-Score and TC-Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610fc708",
   "metadata": {},
   "source": [
    "    [Description]: The methodological quality evaluation (M-Score calculation) of Heatflow database is dependent over the product of T-score and TC-score. The temperature gradient score (T) and thermal conductivity score (TC) is quantified separately for the two different domains of heatflow data collection types (Borehole/Mine and Probing). If the information present about a Heatflow entry is not adequate for score calculation, 'Mx' for added in such scenarios. The T-score or TC-score are originally assigned as 1.0 value. From which addition or deduction are made for each case scenarios with highest penalty. In case of multiple values in a column the code enables highest penalty deduction from T-Score or TC-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6fcdb8",
   "metadata": {},
   "source": [
    "## 3.1. For probe sensing:\n",
    "### 3.1.1. Thermal gradient (T-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b07441",
   "metadata": {},
   "source": [
    "    Description: The case-scenarios in the literature are described comprehensively for estimating the T-score for probing data, which can be found in the mentioned paper on pages 7 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c7fc720",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ProbeT_score(df):    \n",
    "    \n",
    "    Pt = ['P6','C6','C23','C37']\n",
    "    T_score_df = pd.DataFrame()\n",
    "    T_score_df['Error_ProbeTG'] = \"\"\n",
    "    T_score_df['X_ProbeTG'] = \"\"\n",
    "    \n",
    "\n",
    "    for id in df.index:\n",
    "        error_string = \"\"\n",
    "        if df.loc[id,'P12'] in P:\n",
    "            T_score = 1.0\n",
    "            x_present = False\n",
    "            for c in Pt:\n",
    "                p1=p2=p3=p4=least_penalty= None\n",
    "                if c == 'P6':\n",
    "                    #3) the water depth in mbsl (meters below sea level) 'P6'\n",
    "                    if (abs(df.loc[id,'P6']) < 1500) or np.isnan(df.loc[id,'P6']):\n",
    "                        p1 = -0.2\n",
    "                    elif 2500 >= abs(df.loc[id,'P6']) >= 1500:\n",
    "                        p2 = -0.1\n",
    "                    elif (abs(df.loc[id,'P6']) > 2500) or ('[present and corrected]' in df.loc[id,'C17']):\n",
    "                        p3 = 0                            \n",
    "                    else:\n",
    "                        error_string = error_string + f\" {c},\" \n",
    "                        x_present = True\n",
    "\n",
    "                elif c == 'C6':\n",
    "                    #1) Probe penetration depth 'C6'\n",
    "                    if (df.loc[id,'C6'] < 1) or np.isnan(df.loc[id,'C6']):\n",
    "                        p1 = -0.2 \n",
    "                    elif 1 <= df.loc[id,'C6'] <= 3:\n",
    "                        p2 = -0.1\n",
    "                    elif 3 < df.loc[id,'C6'] <= 10:\n",
    "                        p3 = 0\n",
    "                    elif df.loc[id,'C6'] > 10:\n",
    "                        p4 = 0.1\n",
    "                    else:\n",
    "                        error_string = error_string + f\" {c},\"\n",
    "                        x_present = True\n",
    "\n",
    "                elif c == 'C23':\n",
    "                    #4) the tilt of the probe. 'C23'\n",
    "                    if (df.loc[id,'C23'] > 30) or np.isnan(df.loc[id,'C23']):\n",
    "                        p1 = -0.2 \n",
    "                    elif 10 < df.loc[id,'C23'] <= 30:\n",
    "                        p2 = -0.1\n",
    "                    elif (0 < df.loc[id,'C23'] <= 10) or (\"[tilt corrected]\" in df.loc[id,'C11']):\n",
    "                        p3 = 0\n",
    "                    else:\n",
    "                        error_string = error_string + f\" {c},\"\n",
    "                        x_present = True\n",
    "\n",
    "                elif c == 'C37':\n",
    "                    #2) the number of temperature points used to estimate the temperature gradient 'C37'\n",
    "                    if (df.loc[id,'C37'] < 1) or np.isnan(df.loc[id,'C37']):\n",
    "                        p1 = -0.2 \n",
    "                    elif 1 <= df.loc[id,'C37'] <= 3:\n",
    "                        p2 = -0.1\n",
    "                    elif 3 < df.loc[id,'C37'] <= 5:\n",
    "                        p3 = 0\n",
    "                    elif df.loc[id,'C37'] > 5:\n",
    "                        p4 = 0.1\n",
    "                    else:\n",
    "                        error_string = error_string + f\" {c},\"\n",
    "                        x_present = True\n",
    "\n",
    "                least_penalty = min((x for x in [p1,p2,p3,p4] if x is not None), default=0)\n",
    "                T_score = T_score + least_penalty\n",
    "\n",
    "            T_score_df.loc[id,'Error_ProbeTG'] = error_string\n",
    "            T_score_df.loc[id,'X_ProbeTG'] = x_present\n",
    "            T_score_df.loc[id,'Probe_Tscore'] = T_score\n",
    "        else:\n",
    "\n",
    "            T_score_df.loc[id,'Error_ProbeTG'] = error_string\n",
    "            T_score_df.loc[id,'Probe_Tscore'] = np.nan\n",
    "    return T_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a357505",
   "metadata": {},
   "source": [
    "    [Description]: Calling previous functions to prepare data and perform score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23bbf25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Complete_PT_calc(df):\n",
    "    T_score_df = ProbeT_score(change_type(remove_head(df)))\n",
    "    return T_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0264fc9",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1.2. Thermal conductivity (TC-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5534a5",
   "metadata": {},
   "source": [
    "    Description:   The case-scenarios in the literature are described comprehensively for estimating the TC-score for probing data, that can be found in the mentioned paper at page 7 and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d1bdc7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ProbeTC_score(df):   \n",
    "    \n",
    "    Ptc = ['C42','C43','C45','C47']\n",
    "    T_score_df = pd.DataFrame()\n",
    "    T_score_df['Error_ProbeTC'] = \"\"\n",
    "    T_score_df['X_ProbeTC'] = \"\"\n",
    "    \n",
    "    for id in df.index:\n",
    "        error_string = \"\"\n",
    "        \n",
    "        if df.loc[id,'P12'] in P:\n",
    "            T_score = 1.0\n",
    "            x_present = False\n",
    "            \n",
    "            for c in Ptc:\n",
    "                v = df.loc[id,c]\n",
    "                p1=p2=p3=p4=least_penalty= None\n",
    "\n",
    "                if c == 'C42': # tc_location\n",
    "                    if \"[literature/unspecified]\" in df.loc[id,'C42']:\n",
    "                        p1 = -0.2\n",
    "                    elif \"[other location]\" in df.loc[id,'C42']:\n",
    "                        p2 = -0.1\n",
    "                    elif \"[actual heat-flow location]\" in df.loc[id,'C42']:\n",
    "                        p3 = 0 \n",
    "                    else:\n",
    "                        error_string = error_string + f\"{c}, \"\n",
    "                        x_present = True\n",
    "\n",
    "                elif c == 'C43': # tc_method                    \n",
    "                    if df.loc[id,'C43'].startswith('[lab'):\n",
    "                        if df.loc[id,'C44'] in [\"[dry measured]\",\"[unspecified]\",\"[other (specify)]\"]:\n",
    "                            p1 = -0.2\n",
    "                        if \"[saturated calculated]\" in df.loc[id,'C44']:\n",
    "                            p2 = -0.1\n",
    "                        elif df.loc[id,'C44'] in [\"[saturated measured]\",\"[recovered]\"]:\n",
    "                            p3 = 0                              \n",
    "                    elif df.loc[id,'C43'] in [\"[unspecified]\",\"[estimation - from chlorine content]\",\n",
    "                                              \"[estimation - from water content/porosity]\",\n",
    "                                              \"[estimation - from mineral composition]\"]:\n",
    "                        p1 = -0.2\n",
    "                    elif \"[estimation - from lithology and literature]\" in df.loc[id,'C43']:\n",
    "                        p2 = -0.1\n",
    "                    elif \"[probe - pulse technique]\" in df.loc[id,'C43']:\n",
    "                        p4 = 0.1\n",
    "                    else:\n",
    "                        error_string = error_string + f\"{c}, \"\n",
    "                        x_present = True\n",
    "\n",
    "\n",
    "                elif c == 'C45': # tc_pT_conditions\n",
    "                    if df.loc[id,'C45'] in [\"[recorded ambient pt conditions]\",\"[unrecorded ambient pt conditions]\",\n",
    "                                            \"[unspecified]\"]:\n",
    "                        p1 = -0.2 \n",
    "                    elif df.loc[id,'C45'] in [\"[replicated in-situ (p)]\",\"[corrected in-situ (p)]\",\n",
    "                                              \"[replicated in-situ (t)]\",\"[corrected in-situ (t)]\"]:\n",
    "                        p2 = -0.1 \n",
    "                    elif df.loc[id,'C45'] in [\"[replicated in-situ (pt)]\",\"[corrected in-situ (pt)]\"]:\n",
    "                        p3 = 0\n",
    "                    elif (\"[actual in-situ (pt) conditions]\" in df.loc[id,'C45']) and (\"[probe - pulse technique]\"\n",
    "                                                                                       in df.loc[id,'C43']):\n",
    "                        p4 = 0.1    \n",
    "                    else:\n",
    "                        error_string = error_string + f\"{c}, \"\n",
    "                        x_present = True\n",
    "\n",
    "                elif c == 'C47': # tc_number\n",
    "                    if (\"[literature/unspecified]\" not in df.loc[id,'C42']) and ((0<=df.loc[id,'C47']<=1)\n",
    "                                                                                 or np.isnan(df.loc[id,'C47'])):\n",
    "                        p1 = -0.2\n",
    "                    elif (\"[literature/unspecified]\" not in df.loc[id,'C42']) and (2<=df.loc[id,'C47']<=3):\n",
    "                        p2 = -0.1\n",
    "                    elif (\"[literature/unspecified]\" not in df.loc[id,'C42']) and (df.loc[id,'C47']>3):\n",
    "                        p3 = 0   \n",
    "                    else:\n",
    "                        error_string = error_string + f\"{c}, \"\n",
    "                        x_present = True\n",
    "\n",
    "                least_penalty = min((x for x in [p1,p2,p3,p4] if x is not None), default=0)\n",
    "                T_score = T_score + least_penalty\n",
    "            T_score_df.loc[id,'Error_ProbeTC'] = error_string\n",
    "            T_score_df.loc[id,'X_ProbeTC'] = x_present\n",
    "            T_score_df.loc[id,'Probe_TCscore'] = T_score\n",
    "        else:\n",
    "            T_score_df.loc[id,'Error_ProbeTC'] = error_string\n",
    "            T_score_df.loc[id,'Probe_TCscore'] = np.nan\n",
    "    return T_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6c9bab",
   "metadata": {},
   "source": [
    "    [Description]: Calling previous functions to prepare data and perform score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54c3d490",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Complete_PTC_calc(df):\n",
    "    T_score_df = ProbeTC_score(change_type(remove_head(df)))\n",
    "    return T_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38056210",
   "metadata": {},
   "source": [
    "## 3.2. For borehole and mine data:\n",
    "### 3.2.1. Thermal gradient (T-score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b81dd50",
   "metadata": {},
   "source": [
    "    [Description]:   The case-scenarios in the literature are described comprehensively for estimating the T-score for borehole/mine data, that can be found in the mentioned paper at page 7 and 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be15c415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bore_t_M_score(df):\n",
    "    T_score_df = pd.DataFrame()\n",
    "    T_score_df['Error_BoreTG'] = \"\"\n",
    "    T_score_df['X_BoreTG'] = \"\"\n",
    "    \n",
    "    for id in df.index:\n",
    "        error_string = \"\"\n",
    "        \n",
    "        if df.loc[id,'P12'] in B:\n",
    "            T_score = 1.0\n",
    "            x_present = False\n",
    "            p1=p2=p3=least_penalty= None \n",
    "            \n",
    "            if (df.loc[id,'C31'] == \"[sur]\") and (df.loc[id,'C32'] in ['[cpd]', '[xen]', '[gtm]', '[bsr]', '[bht]',\n",
    "                                                                       '[ht-ft]', '[rtdpert]', '[cbht]', '[cht-ft]',\n",
    "                                                                       '[rtdeq]', '[rtdc]', '[oddt-pc]', '[oddt-tp]',\n",
    "                                                                      \"[grt]\",\"[egrt]\"]): # modification\n",
    "                if df.loc[id,'C32'] in ['[cpd]', '[xen]', '[gtm]', '[bsr]'] :\n",
    "                    p1 = -0.6     \n",
    "                elif df.loc[id,'C32'] in ['[bht]', '[ht-ft]', '[rtdpert]']:\n",
    "                    p2 = -0.5\n",
    "                elif df.loc[id,'C32'] in ['[cbht]', '[cht-ft]', '[rtdeq]', '[rtdc]', '[oddt-pc]', '[oddt-tp]',\"[grt]\",\"[egrt]\"]:\n",
    "                    p3 = -0.3\n",
    "                else:\n",
    "                    error_string = error_string + \"C32\"\n",
    "                    x_present = True\n",
    "\n",
    "            elif (df.loc[id,'C37'] > 15) and ((df.loc[id,'C31'] or df.loc[id,'C32']) in ['[logpert]', '[logeq]',\n",
    "                                                                                         '[clog]', '[dtseq]', '[cdts]']):\n",
    "                if '[logpert]' in (df.loc[id,'C31'] or df.loc[id,'C32']):\n",
    "                    p1 = -0.1\n",
    "                elif (df.loc[id,'C31'] or df.loc[id,'C32']) in ['[logeq]', '[clog]', '[dtseq]', '[cdts]']:\n",
    "                    p2 = 0.1\n",
    "                else:\n",
    "                    error_string = error_string + \"C31 or C32\"\n",
    "                    x_present = True\n",
    "                    \n",
    "            elif (15 > df.loc[id,'C37'] > 2) and ((df.loc[id,'C31'] or df.loc[id,'C32']) in ['[cpd]', '[xen]', '[gtm]',\n",
    "                                                                                             '[bsr]', '[logpert]', '[dtspert]',\n",
    "                                                                                             '[bht]', '[ht-ft]', '[rtdpert]',\n",
    "                                                                                             '[blk]', '[logeq]', '[clog]',\n",
    "                                                                                             '[dtseq]', '[cdts]', '[cbht]',\n",
    "                                                                                             '[ht-ft]', '[rtdeq]', '[rtdc]',\n",
    "                                                                                             '[oddt-pc]', '[oddt-tp]',\n",
    "                                                                                            \"[grt]\",\"[egrt]\"]): # modification\n",
    "                if (df.loc[id,'C31'] or df.loc[id,'C32']) in ['[cpd]', '[xen]', '[gtm]', '[bsr]']:\n",
    "                    p1 = -0.5\n",
    "                elif (df.loc[id,'C31'] or df.loc[id,'C32']) in ['[logpert]', '[dtspert]', '[bht]', '[ht-ft]', '[rtdpert]',\n",
    "                                                                '[blk]']:\n",
    "                    p2 = -0.3\n",
    "                elif (df.loc[id,'C31'] or df.loc[id,'C32']) in ['[logeq]', '[clog]', '[dtseq]', '[cdts]', '[cbht]',\n",
    "                                                                '[ht-ft]', '[rtdeq]', '[rtdc]', '[oddt-pc]', '[oddt-tp]',\n",
    "                                                               \"[grt]\",\"[egrt]\"]: # modification\n",
    "                    p3 = -0.1\n",
    "                else:\n",
    "                    error_string = error_string + \"C31 or C32\"\n",
    "                    x_present = True\n",
    "\n",
    "            else:\n",
    "                p1 = -0.6\n",
    "\n",
    "            least_penalty = min((x for x in [p1,p2,p3] if x is not None), default=0)\n",
    "            T_score = T_score + least_penalty\n",
    "            T_score_df.loc[id,'Error_BoreTG'] = error_string\n",
    "            T_score_df.loc[id,'X_BoreTG'] = x_present\n",
    "            T_score_df.loc[id,'Bore_Tscore'] = T_score\n",
    "        else:\n",
    "            T_score_df.loc[id,'Error_BoreTG'] = error_string\n",
    "            T_score_df.loc[id,'Bore_Tscore'] = np.nan\n",
    "    return T_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842ba8e",
   "metadata": {},
   "source": [
    "    [Description]: Calling previous functions to prepare data and perform score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9a04afe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Complete_BtM_calc(df):\n",
    "    T_score_df = Bore_t_M_score(change_type(remove_head(df)))\n",
    "    return T_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25213c22",
   "metadata": {},
   "source": [
    "### 3.2.2. Thermal conductivity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9b7a18",
   "metadata": {},
   "source": [
    "    [Description]:   The case-scenarios in the literature are described comprehensively for estimating the TC-score for borehole/mine data, that can be found in the mentioned paper at page 7 and 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed3e6c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bore_tc_M_score(df):\n",
    "    # Assigning columns to Probe - thermal gradient\n",
    "    Btc = ['C41','C42','C44','C45','C47']\n",
    "    T_score_df = pd.DataFrame()\n",
    "    T_score_df['Error_BoreTC'] = \"\"\n",
    "    T_score_df['X_BoreTC'] = \"\"\n",
    "    \n",
    "    for id in df.index:\n",
    "        error_string = \"\"\n",
    "        \n",
    "        if df.loc[id,'P12'] in B:\n",
    "            T_score = 1.0\n",
    "            x_present = False\n",
    "            \n",
    "            for c in Btc:\n",
    "                v = df.loc[id,c]\n",
    "                if isinstance(v, str) == None:\n",
    "                    T_score = np.nan\n",
    "                    break\n",
    "                p1=p2=p3=p4=least_penalty= None\n",
    "                \n",
    "                if c == 'C42' and ((df.loc[id,'C4'] or df.loc[id,'C5']) is np.nan):\n",
    "                #1) Localization\n",
    "                    T_score = 0.1\n",
    "                    break\n",
    "                    \n",
    "                elif (c == 'C42') and ((df.loc[id,'C4'] or df.loc[id,'C5']) is not np.nan):              \n",
    "                    if \"[literature/unspecified]\" in df.loc[id,'C42']:\n",
    "                        p1 = -0.2\n",
    "                    elif \"[other location]\" in df.loc[id,'C42']:\n",
    "                        p2 = -0.1\n",
    "                    elif \"[actual heat-flow location]\" in df.loc[id,'C42']:\n",
    "                        p3 = 0\n",
    "                    else:\n",
    "                        error_string = error_string + f\" {c},\"\n",
    "                        x_present = True\n",
    "\n",
    "                elif c == 'C41':\n",
    "                # 2) Source type\n",
    "                    if df.loc[id,'C41'] in [\"[mineral computation]\",\"[assumed from literature]\",\"[other (specify)]\",\n",
    "                                            \"[unspecified]\"]:\n",
    "                        p1 = -0.2\n",
    "                    elif df.loc[id,'C41'] in [\"[cutting samples]\",\"[outcrop samples]\",\"[well-log interpretation]\"]:\n",
    "                        p2 = -0.1\n",
    "                    elif \"[core samples]\" in df.loc[id,'C41']:\n",
    "                        p3 = 0\n",
    "                    elif df.loc[id,'C41'] in [\"[in-situ probe]\",\"[core-log integration]\"]:\n",
    "                        p4 = 0.1\n",
    "                    else:\n",
    "                        error_string = error_string + f\" {c},\"\n",
    "                        x_present = True\n",
    "\n",
    "                elif c == 'C47':\n",
    "                # 3) Number of conductivites: tc_number\n",
    "                    if \"[literature/unspecified]\" in df.loc[id,'C42']:\n",
    "                        p1 = -0.1\n",
    "                    elif (\"[literature/unspecified]\" not in df.loc[id,'C42']) and (np.isnan(df.loc[id,'C47'])\n",
    "                                                                                   or (1<=df.loc[id,'C47']<=15)):\n",
    "                        p1 = -0.1\n",
    "                    elif (\"[literature/unspecified]\" not in df.loc[id,'C42']) and (df.loc[id,'C47'] >15):\n",
    "                        p2 = 0\n",
    "                    else:\n",
    "                        error_string = error_string + f\" {c},\"\n",
    "                        x_present = True\n",
    "\n",
    "                elif c == 'C44':\n",
    "                    # 4) Saturation: tc_saturation\n",
    "                    if df.loc[id,'C44'] in [\"[dry measured]\",\"[unspecified]\",\"[other (specify)]\"]:\n",
    "                        p1 = -0.2\n",
    "                    elif df.loc[id,'C44'] in [\"[recovered]\",\"[saturated calculated]\"]:\n",
    "                        p2 = -0.1\n",
    "                    elif df.loc[id,'C44'] in [\"[saturated measured in-situ]\",\"[saturated measured]\"]:\n",
    "                        p3 = 0\n",
    "                    else:\n",
    "                        error_string = error_string + f\" {c},\"\n",
    "                        x_present = True\n",
    "\n",
    "                elif c == 'C45':\n",
    "                    # 4) Pressure temperature: tc_pT_conditions\n",
    "                    if df.loc[id,'C45'] in [\"[recorded ambient pt conditions]\", \"[unrecorded ambient pt conditions]\",\n",
    "                                            \"[unspecified]\"]:\n",
    "                        p1 = -0.2\n",
    "                    elif df.loc[id,'C45'] in [\"[replicated in-situ (p)]\", \"[corrected in-situ (p)]\",\n",
    "                                              \"[replicated in-situ (t)]\", \"[corrected in-situ (t)]\"]:\n",
    "                        p2 = -0.1\n",
    "                    elif df.loc[id,'C45'] in [\"[actual in-situ (pt) conditions]\", \"[replicated in-situ (pt)]\",\n",
    "                                              \"[corrected in-situ (pt)]\"]:\n",
    "                        p3 = 0\n",
    "                    else:\n",
    "                        error_string = error_string + f\" {c},\"\n",
    "                        x_present = True\n",
    "                least_penalty = min((x for x in [p1,p2,p3,p4] if x is not None), default=0)\n",
    "                T_score = T_score + least_penalty\n",
    "            T_score_df.loc[id,'Error_BoreTC'] = error_string\n",
    "            T_score_df.loc[id,'X_BoreTC'] = x_present\n",
    "            T_score_df.loc[id,'Bore_TCscore'] = T_score\n",
    "        else:\n",
    "            T_score_df.loc[id,'Error_BoreTC'] = error_string\n",
    "            T_score_df.loc[id,'Bore_TCscore'] = np.nan\n",
    "    return T_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9e0bee",
   "metadata": {},
   "source": [
    "    [Description]: Calling previous functions to prepare data and perform score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d50229a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Complete_BtcM_calc(df):\n",
    "    T_score_df = Bore_tc_M_score(change_type(remove_head(df)))\n",
    "    return T_score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25736d33",
   "metadata": {},
   "source": [
    "## 3.3 Concatenate results of T scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82caec8",
   "metadata": {},
   "source": [
    "    [Description]: Combining all the score results from different T and TC-score determiniation in single output df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a47dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_TScores(df):\n",
    "    new_df = change_type(remove_head(df))\n",
    "    result = pd.concat([new_df['P12'],Complete_PT_calc(df), Complete_PTC_calc(df), Complete_BtM_calc(df),\n",
    "                        Complete_BtcM_calc(df)], axis=1) # \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc777c60",
   "metadata": {},
   "source": [
    "## 3.4 Combine T and TC score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4398f8ff",
   "metadata": {},
   "source": [
    "    [Description]: Choosing T and TC-scores, depending on which approach: probing or borehole/mine it has been collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a469559e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_Tscore(df):\n",
    "    df['X_Probe'] = df['X_ProbeTG'] | df['X_ProbeTC']\n",
    "    df['X_Bore'] = df['X_BoreTG'] | df['X_BoreTC']\n",
    "    df['X'] = False\n",
    "    df['Error'] = ''\n",
    "    for id in df.index:\n",
    "        #Probe_sensing\n",
    "        if (df.loc[id, 'P12']) in P:\n",
    "            df.loc[id, 'T_score'] = df.loc[id, 'Probe_Tscore']\n",
    "            df.loc[id, 'TC_score'] = df.loc[id, 'Probe_TCscore']\n",
    "            df.loc[id, 'X']= df.loc[id, 'X_Probe']\n",
    "            df.loc[id, 'Error'] = df.loc[id, 'Error_ProbeTG'] + df.loc[id, 'Error_ProbeTC']\n",
    "        #Borehole/ mining\n",
    "        elif (df.loc[id, 'P12']) in B:\n",
    "            df.loc[id, 'T_score'] = df.loc[id, 'Bore_Tscore']\n",
    "            df.loc[id, 'TC_score'] = df.loc[id, 'Bore_TCscore']\n",
    "            df.loc[id, 'X']= df.loc[id, 'X_Bore']\n",
    "            df.loc[id, 'Error'] = df.loc[id, 'Error_BoreTG'] + df.loc[id, 'Error_BoreTC']\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    column_to_remove = ['Probe_Tscore', 'Probe_TCscore', 'Bore_Tscore', 'Bore_TCscore','Error_ProbeTG',\n",
    "                        'Error_ProbeTC','Error_BoreTG','Error_BoreTC','X_Probe','X_Bore','X_ProbeTG',\n",
    "                        'X_ProbeTC','X_BoreTG','X_BoreTC']\n",
    "    df = df.drop(column_to_remove, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7bf12c",
   "metadata": {},
   "source": [
    "## 3.5 Calculate Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3189fe77",
   "metadata": {},
   "source": [
    "    [Description]: Calculate the quality product of the T and TC-scores. The graphical representation of this can be found in Fig. 3 in the paper mentioned above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "515fc5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def T_quality(df):\n",
    "    df['T_Quality'] = np.nan\n",
    "    for id in df.index:\n",
    "        if np.isnan(df.loc[id,'T_score']):\n",
    "            df.loc[id,'T_Quality'] = df.loc[id,'TC_score']\n",
    "        elif np.isnan(df.loc[id,'TC_score']):\n",
    "            df.loc[id,'T_Quality'] = df.loc[id,'T_score']\n",
    "        else:\n",
    "            df.loc[id,'T_Quality'] = df.loc[id,'T_score'] * df.loc[id,'TC_score']        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0055c20d",
   "metadata": {},
   "source": [
    "# 4. Calculating M Score "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09b71d3",
   "metadata": {},
   "source": [
    "    [Description]: Determine the M-Score based on the quality assessed above. The scores range from M1 to M4, with M1 being the best quality and M4 the lowest. M-Score with a 'x' indicates inadequacy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e9561a",
   "metadata": {},
   "source": [
    "![M-Score Image](Graphics\\M-score.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9e6cd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def M_score(df):\n",
    "    df['M_Score'] = ''\n",
    "    for id in df.index:\n",
    "        dfvalue = df.loc[id,'T_Quality']\n",
    "        if 1.5000>dfvalue>=0.7500:\n",
    "            if np.isnan(df.loc[id,'TC_score']) or np.isnan(df.loc[id,'T_score']) or (df.loc[id,'X']==True):\n",
    "                df.loc[id,'M_Score'] = 'M1x'\n",
    "            else:\n",
    "                df.loc[id,'M_Score'] = 'M1'\n",
    "\n",
    "        elif 0.7500>dfvalue>=0.5000:\n",
    "            if np.isnan(df.loc[id,'TC_score']) or np.isnan(df.loc[id,'T_score']) or (df.loc[id,'X']==True):\n",
    "                df.loc[id,'M_Score'] = 'M2x'\n",
    "            else:\n",
    "                df.loc[id,'M_Score'] = 'M2'\n",
    "\n",
    "        elif 0.5000>dfvalue>=0.2500:\n",
    "            if np.isnan(df.loc[id,'TC_score']) or np.isnan(df.loc[id,'T_score']) or (df.loc[id,'X']==True):\n",
    "                df.loc[id,'M_Score'] = 'M3x'\n",
    "            else:\n",
    "                df.loc[id,'M_Score'] = 'M3'\n",
    "\n",
    "        elif 0.2500>dfvalue>=0:\n",
    "            if np.isnan(df.loc[id,'TC_score']) or np.isnan(df.loc[id,'T_score']) or (df.loc[id,'X']==True):\n",
    "                df.loc[id,'M_Score'] = 'M4x'\n",
    "            else:\n",
    "                df.loc[id,'M_Score'] = 'M4'\n",
    "        else:\n",
    "            df.loc[id,'M_Score'] = 'Mx'\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aecfff2",
   "metadata": {},
   "source": [
    "    [Description]: Calling previous functions to prepare data and perform score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e3ed55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_MScore_calc(df):\n",
    "    result = M_score(T_quality(merge_Tscore(concatenate_TScores(df))))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cecff03",
   "metadata": {},
   "source": [
    "# 5. Calculating P Flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe02e06",
   "metadata": {},
   "source": [
    "    [Description]: The coding of perturbation effects (p-flags) is detailed in the mentioned paper on pages 8 and 9. The function 'p_flag()' checks all the relevant columns, 'C13' to 'C19' values for the same."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a630bce",
   "metadata": {},
   "source": [
    "![P-flags Image](Graphics\\P-flags.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1d8272a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_flag(df):\n",
    "    StrC = ['C13','C14','C15','C16','C17','C18','C19']\n",
    "    PFlag_df = pd.DataFrame()\n",
    "    PFlag_df['P_Flag'] = \"\"\n",
    "    \n",
    "    for id in df.index:\n",
    "        Pflag = ''\n",
    "        for c in StrC:\n",
    "            # sedimentation (S/s) - C13\n",
    "            if c == 'C13':                \n",
    "                if '[present and corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'S'\n",
    "                elif '[present and not corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'s'\n",
    "                elif '[present not significant]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'X'\n",
    "                elif ('[not recognised]' in df.loc[id,c]) or ('[not recognized]' in df.loc[id,c]):\n",
    "                    Pflag = Pflag+'x'\n",
    "                else:\n",
    "                    Pflag = Pflag+'-'\n",
    "            # erosion (E/e) - C14\n",
    "            elif c == 'C14':                \n",
    "                if '[present and corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'E'\n",
    "                elif '[present and not corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'e'\n",
    "                elif '[present not significant]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'X'\n",
    "                elif ('[not recognised]' in df.loc[id,c]) or ('[not recognized]' in df.loc[id,c]):\n",
    "                    Pflag = Pflag+'x'\n",
    "                else:\n",
    "                    Pflag = Pflag+'-'\n",
    "            # topography/bathymetry (T/t) - C15\n",
    "            elif c == 'C15':                \n",
    "                if '[present and corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'T'\n",
    "                elif '[present and not corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'t'\n",
    "                elif '[present not significant]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'X'\n",
    "                elif ('[not recognised]' in df.loc[id,c]) or ('[not recognized]' in df.loc[id,c]):\n",
    "                    Pflag = Pflag+'x'\n",
    "                else:\n",
    "                    Pflag = Pflag+'-'\n",
    "            # paleoclimate/glaciation(P/p) - C16\n",
    "            elif c == 'C16':                \n",
    "                if '[present and corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'P'\n",
    "                elif '[present and not corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'p'\n",
    "                elif '[present not significant]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'X'\n",
    "                elif ('[not recognised]' in df.loc[id,c]) or ('[not recognized]' in df.loc[id,c]):\n",
    "                    Pflag = Pflag+'x'\n",
    "                else:\n",
    "                    Pflag = Pflag+'-'\n",
    "            # surface/bottom water temperature variations (V/v) - C17\n",
    "            elif c == 'C17':                \n",
    "                if '[present and corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'V'\n",
    "                elif '[present and not corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'v'\n",
    "                elif '[present not significant]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'X'\n",
    "                elif ('[not recognised]' in df.loc[id,c]) or ('[not recognized]' in df.loc[id,c]):\n",
    "                    Pflag = Pflag+'x'\n",
    "                else:\n",
    "                    Pflag = Pflag+'-'\n",
    "            # convection/fluid flow/hydrate dynamics (C/c) - C18\n",
    "            elif c == 'C18':                \n",
    "                if '[present and corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'C'\n",
    "                elif '[present and not corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'c'\n",
    "                elif '[present not significant]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'X'\n",
    "                elif ('[not recognised]' in df.loc[id,c]) or ('[not recognized]' in df.loc[id,c]):\n",
    "                    Pflag = Pflag+'x'\n",
    "                else:\n",
    "                    Pflag = Pflag+'-'\n",
    "            # Structural effects: heat refraction (R/r) - C19\n",
    "            elif c == 'C19':                \n",
    "                if '[present and corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'R'\n",
    "                elif '[present and not corrected]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'r'\n",
    "                elif '[present not significant]' in df.loc[id,c]:\n",
    "                    Pflag = Pflag+'X'\n",
    "                elif ('[not recognised]' in df.loc[id,c]) or ('[not recognized]' in df.loc[id,c]):\n",
    "                    Pflag = Pflag+'x'\n",
    "                else:\n",
    "                    Pflag = Pflag+'-'\n",
    "                    \n",
    "        PFlag_df.loc[id,'P_Flag'] = Pflag\n",
    "    return PFlag_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34d4c04",
   "metadata": {},
   "source": [
    "    [Description]: Calling previous functions to prepare data and perform score evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2d874c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_PFlag_calc(df):\n",
    "    result_df = p_flag(change_type(remove_head(df)))\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c3009f",
   "metadata": {},
   "source": [
    "# 6. Combined Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78a67b",
   "metadata": {},
   "source": [
    "    [Description]: Combining all the scores: U, M and P-flags together as a final output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843412c9",
   "metadata": {},
   "source": [
    "![flowchart Image](Graphics\\flowchart.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "35a0287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_score(df):\n",
    "    result = pd.DataFrame()\n",
    "    UScore = CompleteUscore_calc(df)\n",
    "    MScore = complete_MScore_calc(df)\n",
    "    Pflag = complete_PFlag_calc(df)\n",
    "    result['U_score'] = UScore['U_score']\n",
    "    result['M_score'] = MScore['M_Score']\n",
    "    result['P_flags'] = Pflag['P_Flag']\n",
    "    result['Combined_score'] = UScore['U_score'].astype(str) + MScore['M_Score'].astype(str) + '.' + Pflag['P_Flag'].astype(str)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17582cf7",
   "metadata": {},
   "source": [
    "# 7. Attach to original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a29b242",
   "metadata": {},
   "source": [
    "    [Description]: Attaching the combined results column to the original database with correct indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c271caa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attachOG(og):\n",
    "    result = combined_score(og)\n",
    "    if og.at[0, 'ID'] == 'Obligation':\n",
    "        result_structure_data = {\n",
    "            \"U_score\": ['-', '-', '-', 'uncertainty quantification', '-', '-', 'U-score'],\n",
    "            \"M_score\": ['-', '-', '-', 'methodological quality', '-', '-', 'M-score'],\n",
    "            \"P_flags\": ['-', '-', '-', 'perturbations effects', '-', '-', 'P-flags'],\n",
    "            \"Combined_score\": ['-', '-', '-', 'quality code', '-', '-', 'Quality_Code']\n",
    "        }\n",
    "        result_structure = pd.DataFrame(result_structure_data)\n",
    "        \n",
    "        result = pd.concat([result_structure, result], ignore_index=True)\n",
    "        \n",
    "    elif og.at[0, 'ID'] == 'Short Name':\n",
    "        result_structure_data = {\n",
    "            \"U_score\": ['U-score'],\n",
    "            \"M_score\": ['M-score'],\n",
    "            \"P_flags\": ['P-flags'],\n",
    "            \"Combined_score\": ['Quality_Code']\n",
    "        }\n",
    "        result_structure = pd.DataFrame(result_structure_data)\n",
    "        \n",
    "        result = pd.concat([result_structure, result], ignore_index=True)\n",
    "    \n",
    "    og = pd.merge(og, result[['Combined_score','U_score','M_score','P_flags']], left_index=True, right_index=True, how='left')\n",
    "    \n",
    "    og.rename(columns={\"Combined_score\": \"A9\", \n",
    "                       \"U_score\": \"A10\", \n",
    "                       \"M_score\": \"A11\", \n",
    "                       \"P_flags\": \"A12\"}, inplace=True)\n",
    "    \n",
    "    return og"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781aed3b-6790-4ea0-b896-4dbcd6808793",
   "metadata": {},
   "source": [
    "# 8. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad9d89c",
   "metadata": {},
   "source": [
    "## 8.1. Results of all files in a folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2122674",
   "metadata": {},
   "source": [
    "    [Description]: To generate results for all the Heatflow database in a folder stored in .csv format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "900192ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def folder_result(folder_path):\n",
    "\n",
    "    csv_files = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "    \n",
    "    for csv_file_path in csv_files:\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        \n",
    "        df_result = attachOG(df)\n",
    "        \n",
    "        output_excel_file = os.path.splitext(csv_file_path)[0] + '_scores_result.xlsx'\n",
    "        \n",
    "        df_result.to_excel(output_excel_file, index=False)\n",
    "        \n",
    "        print(f\"Result exported. Excel file saved as: {output_excel_file}\")\n",
    "\n",
    "    for csv_file_path in csv_files:\n",
    "        os.remove(csv_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0064ba",
   "metadata": {},
   "source": [
    "# 9. hfqa_tool function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960e6d02",
   "metadata": {},
   "source": [
    "     [Description]: To calculate Quality score for all the HF dataframe files in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2cb8b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_score():\n",
    "    folder_path = input(\"Please enter the file directory for score calculation: \")\n",
    "    readable(folder_path)\n",
    "    folder_result(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1dd074e7-1f60-4e3b-88ef-812213162943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result exported. Excel file saved as: Z:\\Tempfeld\\WG\\PROJEKTE\\P_Heat-Flow\\databases\\_DB_IHFC_Update_2025\\HiWi Area 2025\\Saman\\check_florian\\Dello-Iacovo_2014\\Dello-Iacovo_2014_scores_result.xlsx\n"
     ]
    }
   ],
   "source": [
    "quality_score()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Heatflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
